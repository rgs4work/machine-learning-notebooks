{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80749867",
   "metadata": {},
   "source": [
    "Prioritized Experience Replay is a type of experience replay in reinforcement learning where we In more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error.\n",
    "\n",
    "https://arxiv.org/abs/1312.5602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663b32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d586dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self,max_memory_size):\n",
    "        self.replay_memory = deque(maxlen=max_memory_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        '''Adds tuple(s,a,r,ns,d) to the replay memory deque'''\n",
    "        self.replay_memory.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.replay_memory) > batch_size:\n",
    "            batch = random.choices(self.replay_memory, k=batch_size)\n",
    "        else:\n",
    "            batch = self.replay_memory     \n",
    "        return batch\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class PrioritizedReplayMemory:\n",
    "    def __init__(self,max_memory_size):\n",
    "        self.replay_memory = deque(maxlen=max_memory_size)\n",
    "        self.priorities = deque(maxlen=max_memory_size)\n",
    "        \n",
    "        self.epsilon = 1e-8\n",
    "        self.alpha = 0.5 #(if 0 - random sampling)\n",
    "        \n",
    "        \n",
    "    def get_probabilities(self):\n",
    "        scaled_priorities = np.array(self.priorities) ** self.alpha\n",
    "        probability_batch = scaled_priorities / sum(scaled_priorities)\n",
    "        return probability_batch\n",
    "    \n",
    "        \n",
    "    def add(self, experience):\n",
    "        '''Adds tuple(s,a,r,ns,d) to the replay memory deque'''\n",
    "        self.replay_memory.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.replay_memory) > batch_size:\n",
    "            batch = random.choices(self.replay_memory, k=batch_size)\n",
    "        else:\n",
    "            batch = self.replay_memory     \n",
    "        return batch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40d5fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritized_experience_replay(Q_predicted,Q_target,priority_list):\n",
    "    epsilon = 1e-8\n",
    "    alpha = 0.5 #(if 0 - random sanmpling)\n",
    "    \n",
    "    error  = Q_predicted - Q_target\n",
    "    p_i = error + epsilon\n",
    "    prob_i = (error + epsilon)**alpha/sum(np.array(priority_list)**alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
