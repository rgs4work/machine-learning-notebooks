{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc17ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa291184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.multiply(x, (1 - x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(int)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def linear(x, derivative=False):\n",
    "    if derivative:\n",
    "        return 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def mse(target, output):\n",
    "    return np.square(target - output).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "08e85a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def proceed(self, weights, biases, gradients, gradients_b):        \n",
    "        for n in range(1, len(weights)):           \n",
    "            weights[n] = weights[n] - self.lr * gradients[n]\n",
    "            biases[n] = biases[n] - self.lr * gradients_b[n]        \n",
    "        return weights, biases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3efca8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, in_features, out_features, activation=None, input_layer=False):\n",
    "        self.neurons = out_features        \n",
    "        self.activation = activation\n",
    "        self.input_layer = input_layer\n",
    "\n",
    "        self.weights = self.init_weights(in_features, out_features)\n",
    "        self.biases = self.init_biases(in_features, out_features)\n",
    "\n",
    "    def init_weights(self, in_features, out_features):\n",
    "        if self.input_layer:\n",
    "            return None\n",
    "\n",
    "        edge = np.sqrt(1/in_features)\n",
    "        weights = np.random.uniform(-edge, edge, (out_features, in_features))\n",
    "        return weights\n",
    "\n",
    "    def init_biases(self, in_features, out_features):\n",
    "        if self.input_layer:\n",
    "            return None\n",
    "\n",
    "        edge = np.sqrt(1/in_features)\n",
    "        biases = np.random.uniform(-edge, edge, (out_features, 1))\n",
    "        return biases\n",
    "\n",
    "    def feedforward(self, in_signal):\n",
    "        if self.input_layer:\n",
    "            return in_signal\n",
    "\n",
    "        out_signal = self.activation(np.dot(self.weights, in_signal) + self.biases)\n",
    "        return out_signal    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "accf7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet1:\n",
    "    def __init__(self, neurons, activations, loss, optimizer):\n",
    "        self.layers = self.create_net(neurons, activations)\n",
    "        self.loss_function = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def create_net(self, neurons, activations):\n",
    "        input_layer = Layer(None, neurons[0], input_layer=True)\n",
    "        layers = [input_layer]\n",
    "\n",
    "        for n in range(1, len(neurons)):\n",
    "            layers.append(Layer(neurons[n-1], neurons[n], activations[n]))\n",
    "        return layers\n",
    "\n",
    "    def feedforward(self, input_signal):\n",
    "        outputs = [input_signal]\n",
    "\n",
    "        for i in range(1, len(self.layers)):\n",
    "            outputs.append(self.layers[i].feedforward(outputs[i-1]))\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, input_signal):\n",
    "        prediction = self.feedforward(input_signal)[-1]\n",
    "        return prediction\n",
    "\n",
    "    def back_propagation(self, outputs, target):\n",
    "        \"\"\"The delta rule for single-layered neural networks is a gradient descent method, \n",
    "        using the derivative of the networkâ€™s weights with respect to the output error to \n",
    "        adjust the weights to better classify training examples.\"\"\"\n",
    "        batch_size = target.shape[1]\n",
    "        \n",
    "        weight_gradients = [None for i in range(len(self.layers))]\n",
    "        biase_gradients =  [None for i in range(len(self.layers))]\n",
    "        deltas = [None for i in range(len(self.layers))]\n",
    "\n",
    "        loss = self.loss_function(target, outputs[-1])\n",
    "        \n",
    "        # Output Layer Error and Delta\n",
    "        output_error = -(2/len(target))*(target - outputs[-1])        \n",
    "        deltas[-1] = output_error * self.layers[-1].activation(outputs[-1], derivative=True)\n",
    "        \n",
    "        # Hidden Layers' Errors and Deltas\n",
    "        for i in range(-2, -len(self.layers),-1):\n",
    "            error = np.dot(self.layers[i+1].weights.T, deltas[i+1])\n",
    "            deltas[i] = error * self.layers[i].activation(outputs[i], derivative=True)\n",
    "\n",
    "        \n",
    "        for i in range(-1, -len(self.layers),-1):\n",
    "            weight_gradients[i] = np.dot(deltas[i],  outputs[i-1].T) / batch_size\n",
    "            biase_gradients[i] =  np.mean(deltas[i], axis=1, keepdims=True)\n",
    "            \n",
    "        return weight_gradients, biase_gradients\n",
    "    \n",
    "    \n",
    "    def update_weights(self, gradients, gradients_b):\n",
    "        '''Returns updated weights'''\n",
    "        self.optimizer.proceed(self.weights,self.biases, gradients, gradients_b)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def print_params(self):\n",
    "        for layer in self.layers:\n",
    "            if not layer.input_layer:\n",
    "                print(layer.weights)\n",
    "                print(layer.biases, \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "07760c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  array([[-0.00415042, -0.00378314],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.00063403, -0.00057793]]),\n",
       "  array([[-0.00673358,  0.        , -0.00246692],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.00257515,  0.        ,  0.00094343],\n",
       "         [ 0.        ,  0.        ,  0.        ]]),\n",
       "  array([[ 0.02082788,  0.        , -0.00093487,  0.        ]])],\n",
       " [None,\n",
       "  array([[ 0.01414454],\n",
       "         [ 0.        ],\n",
       "         [-0.00137372]]),\n",
       "  array([[-0.01296496],\n",
       "         [ 0.        ],\n",
       "         [ 0.00495823],\n",
       "         [ 0.        ]]),\n",
       "  array([[0.02707265]])])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1 = NeuralNet1(neurons=[2, 3, 4, 1],\n",
    "                 activations=[None, relu, relu, sigmoid],\n",
    "                 loss=mse,\n",
    "                 optimizer=SGD(0.1))\n",
    "\n",
    "# nn1.print_params()\n",
    "# print(nn1.layers[2].activation)\n",
    "input_signal = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "target_signal = np.array([[0], [1], [1], [0]]).T\n",
    "\n",
    "\n",
    "\n",
    "ff = nn1.feedforward(input_signal)\n",
    "nn1.back_propagation(ff,target_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d0f7a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-2\n",
      "-3\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1 , -4,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "0becf62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03576583 0.95733211 0.96020589 0.05933778]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, layers, activations, loss):\n",
    "        self.layers = layers\n",
    "        self.layers_num = len(self.layers)        \n",
    "\n",
    "        self.activations = activations\n",
    "        self.loss_function = loss\n",
    "\n",
    "        self.weights, self.biases = self.init_weights()\n",
    "        self.outputs = [None for o in range(self.layers_num)]\n",
    "\n",
    "        self.gradients = [None for g in range(self.layers_num)]\n",
    "        self.gradients_b = [None for b in range(self.layers_num)]\n",
    "        \n",
    "        self.lr = 0.1\n",
    "        self.optimizer = SGD(self.lr)\n",
    "\n",
    "        \n",
    "    def init_weights(self):\n",
    "        '''Creates weights for the Neural Net'''\n",
    "        weights = [None for w in range(self.layers_num)]\n",
    "        biases = [None for b in range(self.layers_num)]\n",
    "\n",
    "        for n in range(1, self.layers_num):\n",
    "            edge = np.sqrt(1/self.layers[n-1])\n",
    "\n",
    "            # shape (layer, Previous Layer)\n",
    "            weights[n] = np.random.uniform(-edge, edge,\n",
    "                                           (self.layers[n], self.layers[n-1]))\n",
    "\n",
    "            # Bias shape(h,1) - only 1 Bias neuron per layer\n",
    "            biases[n] = np.random.uniform(-edge, edge, (self.layers[n], 1))\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    \n",
    "    def feedforward(self, input_signal):\n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "        self.outputs[0] = input_signal\n",
    "\n",
    "        for n in range(1, self.layers_num):\n",
    "            self.outputs[n] = self.activations[n](np.dot(self.weights[n], self.outputs[n-1]) + self.biases[n])\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    \n",
    "    def back_propagation(self, target, batch_size):\n",
    "        loss = self.loss_function(target, self.outputs[-1])\n",
    "\n",
    "        error_o = -(2/len(target))*(target - self.outputs[-1])\n",
    "\n",
    "        delta_o = error_o *                              self.activations[-1](self.outputs[-1], derivative=True)\n",
    "\n",
    "        delta_h = np.dot(self.weights[-1].T, delta_o) *  self.activations[-2](self.outputs[-2], derivative=True)\n",
    "\n",
    "        delta_h0 = np.dot(self.weights[-2].T, delta_h) * self.activations[-3](self.outputs[-3], derivative=True)\n",
    "\n",
    "        self.gradients[-1] = np.dot(delta_o,  self.outputs[-2].T) / batch_size\n",
    "        self.gradients_b[-1] = np.mean(delta_o, axis=1, keepdims=True)\n",
    "\n",
    "        self.gradients[-2] = np.dot(delta_h,  self.outputs[-3].T) / batch_size\n",
    "        self.gradients_b[-2] = np.mean(delta_h, axis=1, keepdims=True)\n",
    "\n",
    "        self.gradients[-3] = np.dot(delta_h0,  self.outputs[-4].T) / batch_size\n",
    "        self.gradients_b[-3] = np.mean(delta_h0, axis=1, keepdims=True)\n",
    "\n",
    "        return self.gradients, self.gradients_b\n",
    "\n",
    "    \n",
    "    def update_weights(self, gradients, gradients_b):\n",
    "        '''Returns updated weights'''\n",
    "        self.weights, self.biases = self.optimizer.optimize(self.weights,\n",
    "                                                            self.biases, \n",
    "                                                            gradients,\n",
    "                                                            gradients_b)\n",
    "        \n",
    "        \n",
    "    def predict(self, input_signal):\n",
    "        '''Returns only Output Layer array'''\n",
    "        # dot((R1,C1),(R2,C2)) output shape: (R1,C2), C1 == R2\n",
    "        prediction = self.feedforward(input_signal)[-1]\n",
    "        return prediction\n",
    "\n",
    "    def print_params(self):\n",
    "        print(self.weights[1:])\n",
    "        print(self.biases[1:])\n",
    "        print(self.outputs[1:])\n",
    "\n",
    "\n",
    "nn = NeuralNet(layers=[2, 25, 20, 1],\n",
    "               activations=[None, relu, relu, sigmoid],\n",
    "               loss=mse)\n",
    "\n",
    "\n",
    "input_signal = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "target_signal = np.array([[0], [1], [1], [0]]).T\n",
    "\n",
    "# print(np.shape(target_signal))\n",
    "for epochs in range(1000):\n",
    "    nn.feedforward(input_signal)\n",
    "    gradients = nn.back_propagation(target_signal, 4)\n",
    "    nn.update_weights(*gradients)\n",
    "print(nn.predict(input_signal))\n",
    "\n",
    "\n",
    "# nn.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e1eebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(f):\n",
    "    return np.multiply(f, (1 - f))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(f):\n",
    "    return 1 - np.power(np.tanh(f), 2)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(int)\n",
    "\n",
    "\n",
    "def noneActivation(x):\n",
    "    return x    \n",
    "\n",
    "\n",
    "def noneActivation_derivative(z):\n",
    "    return 1    \n",
    "\n",
    "\n",
    "def mean_squared_error(target, output):\n",
    "    # print('\\n Prediction', output)\n",
    "    # print(target)\n",
    "    return np.square(target - output).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyNeuralNet:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons):\n",
    "        \n",
    "        # Neural Net Layers and Nodes\n",
    "        self.i = input_neurons  #\n",
    "        self.h = hidden_neurons  #\n",
    "        self.o = output_neurons  #\n",
    "        \n",
    "        # Activation functions and derivatives\n",
    "        self.activation_h =  relu\n",
    "        self.activation_o =  sigmoid\n",
    "\n",
    "        self.activation_derivative_h =  relu_derivative\n",
    "        self.activation_derivative_o =  sigmoid_derivative\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_function = mean_squared_error\n",
    "        self.loss = 0\n",
    "        self.loss_list = []\n",
    "        \n",
    "        # Optimizing\n",
    "        self.lr = 0.1\n",
    "        self.optimizer = 'SGD'\n",
    "        \n",
    "        # ADAM Optimizer variables\n",
    "        self.l2_m = 0\n",
    "        self.l1_m = 0\n",
    "        self.l3_m = 0\n",
    "        \n",
    "        self.l2_v = 0\n",
    "        self.l1_v = 0\n",
    "        self.l3_v = 0\n",
    "\n",
    "        self.lb2_m = 0\n",
    "        self.lb1_m = 0\n",
    "        self.lb3_m = 0\n",
    "\n",
    "        self.lb2_v = 0\n",
    "        self.lb1_v = 0\n",
    "        self.lb3_v = 0\n",
    "\n",
    "        self.t = 0\n",
    "        \n",
    "    \n",
    "    def init_weights(self):     \n",
    "        '''Creates weights for the Neural Net'''\n",
    "        # shape (layer, Previous Layer)\n",
    "        self.weight_h =  np.random.uniform(-np.sqrt(1/self.i) , np.sqrt(1/self.i) ,(self.h, self.i))\n",
    "        self.weight_o = np.random.uniform(-np.sqrt(1/self.h) , np.sqrt(1/self.h) ,(self.o, self.h))\n",
    "        # Bias shape(h,1) - only 1 Bias neuron per layer\n",
    "        self.bias_h =  np.random.uniform(-np.sqrt(1/self.i) , np.sqrt(1/self.i) ,(self.h,1))\n",
    "        self.bias_o =  np.random.uniform(-np.sqrt(1/self.h) , np.sqrt(1/self.h) ,(self.o, 1))\n",
    "\n",
    "\n",
    "    def feedforward(self, input_signal):\n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "        # dot((R1,C1),(R2,C2)) output shape: (R1,C2), C1 == R2\n",
    "        output_i = input_signal\n",
    "#         output_i = np.multiply(input_signal,np.random.binomial(1, 1, input_signal.shape)) / np.full(input_signal.shape,0.9)\n",
    "                               \n",
    "        output_h = self.activation_h(np.dot(self.weight_h, output_i) + self.bias_h)\n",
    "#         output_h = np.multiply(output_h, np.random.binomial(1, 1, output_h.shape)) / np.full(output_h.shape,1) # DROPOUT\n",
    "       \n",
    "        output_o = self.activation_o(np.dot(self.weight_o, output_h) + self.bias_o)\n",
    "\n",
    "        return output_i, output_h, output_o #returns tuple\n",
    "    \n",
    "    \n",
    "\n",
    "    def back_propagation(self, output_i, output_h, output_o, target, batch_size):   \n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "        # Target shape : ( output layer size , batch_size)\n",
    "        # for graph\n",
    "        self.loss = self.loss_function(target, output_o)\n",
    "#         if random.uniform(0, 1) > 0.05:\n",
    "#             self.loss_list.append(self.loss)  \n",
    "        \n",
    "\n",
    "        error_o = -(2/len(target))*(target - output_o)\n",
    "       \n",
    "        delta_o =  error_o * self.activation_derivative_o(output_o) \n",
    "        \n",
    "        grad_o = np.dot(delta_o,  output_h.T) / batch_size     # Correct\n",
    "        grad_bias_o = np.mean(delta_o, axis=1, keepdims=True)   # Correct \n",
    "        \n",
    "  \n",
    "        delta_h = np.dot(self.weight_o.T, delta_o) * self.activation_derivative_h(output_h)        \n",
    "        \n",
    "        grad_h = np.dot(delta_h, output_i.T) / batch_size    # Correct\n",
    "        grad_bias_h = np.mean(delta_h, axis=1, keepdims=True)  # Correct  \n",
    "     \n",
    "        return grad_h ,grad_o, grad_bias_h, grad_bias_o\n",
    "\n",
    "    \n",
    "    \n",
    "    def update_weights(self, grad_h ,grad_o, grad_bias_h, grad_bias_o):\n",
    "        '''Returns (weight_h, weight_o, weight_bias_h,  weight_bias_o)'''             \n",
    "                \n",
    "        if self.optimizer == 'SGD':\n",
    "            self.sgd_optimizer( grad_h, grad_o, grad_bias_h, grad_bias_o)\n",
    "\n",
    "        elif self.optimizer == 'ADAM':\n",
    "            self.adam_optimizer(grad_h, grad_o, grad_bias_h, grad_bias_o)\n",
    "\n",
    "\n",
    "    def predict(self, input_signal):\n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "        # dot((R1,C1),(R2,C2)) output shape: (R1,C2), C1 == R2\n",
    "        \n",
    "        output_i = input_signal\n",
    "        output_h = self.activation_h(np.dot(self.weight_h, output_i) + self.bias_h)\n",
    "        output_o = self.activation_o(np.dot(self.weight_o, output_h) + self.bias_o)\n",
    "\n",
    "        return output_o #returns tuple\n",
    "    \n",
    "#         '''Returns only Output Layer array'''\n",
    "#         _, _, output_o = self.feedforward(input_signal)\n",
    "#         return output_o\n",
    "    \n",
    "\n",
    "        \n",
    "    def sgd_optimizer(self, grad_h, grad_o, grad_bias_h, grad_bias_o):\n",
    "        # Update Weights\n",
    "        self.weight_o = self.weight_o - self.lr * grad_o\n",
    "        self.weight_h = self.weight_h - self.lr * grad_h\n",
    "\n",
    "        self.bias_h = self.bias_h - self.lr * grad_bias_h\n",
    "        self.bias_o = self.bias_o - self.lr * grad_bias_o\n",
    "\n",
    "\n",
    "    def adam_optimizer(self, grad_h, grad_o, grad_bias_h, grad_bias_o):\n",
    "        decay_rate_1 = 0.9\n",
    "        decay_rate_2 = 0.999\n",
    "        epsilon = 10**(-8)\n",
    "\n",
    "        g2 = grad_o        \n",
    "        g0 = grad_h\n",
    "\n",
    "        gb2 = grad_bias_o\n",
    "        gb0 = grad_bias_h\n",
    "\n",
    "        self.t += 1  # Increment Time Step\n",
    "\n",
    "        # Computing 1st and 2nd moment for each layer\n",
    "        self.l3_m = self.l3_m * decay_rate_1 + (1 - decay_rate_1) * g2\n",
    "        self.l1_m = self.l1_m * decay_rate_1 + (1 - decay_rate_1) * g0\n",
    "        \n",
    "        self.l3_v = self.l3_v * decay_rate_2 + (1 - decay_rate_2) * np.square(g2)        \n",
    "        self.l1_v = self.l1_v * decay_rate_2 + (1 - decay_rate_2) * np.square(g0)\n",
    "        \n",
    "        self.lb3_m = self.lb3_m * decay_rate_1 + (1 - decay_rate_1) * gb2        \n",
    "        self.lb1_m = self.lb1_m * decay_rate_1 + (1 - decay_rate_1) * gb0\n",
    "        \n",
    "        self.lb3_v = self.lb3_v * decay_rate_2 + (1 - decay_rate_2) * np.square(gb2)        \n",
    "        self.lb1_v = self.lb1_v * decay_rate_2 + (1 - decay_rate_2) * np.square(gb0)\n",
    "\n",
    "        # Computing bias-corrected moment\n",
    "        l3_m_corrected = self.l3_m / (1 - (decay_rate_1 ** self.t))\n",
    "        l3_v_corrected = self.l3_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        l1_m_corrected = self.l1_m / (1 - (decay_rate_1 ** self.t))\n",
    "        l1_v_corrected = self.l1_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        lb3_m_corrected = self.lb3_m / (1 - (decay_rate_1 ** self.t))\n",
    "        lb3_v_corrected = self.lb3_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        lb1_m_corrected = self.lb1_m / (1 - (decay_rate_1 ** self.t))\n",
    "        lb1_v_corrected = self.lb1_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        # Update Weights\n",
    "        w2_update = l3_m_corrected / (np.sqrt(l3_v_corrected) + epsilon)        \n",
    "        w0_update = l1_m_corrected / (np.sqrt(l1_v_corrected) + epsilon)\n",
    "        b2_update = lb3_m_corrected / (np.sqrt(lb3_v_corrected) + epsilon)        \n",
    "        b0_update = lb1_m_corrected / (np.sqrt(lb1_v_corrected) + epsilon)\n",
    "\n",
    "        self.weight_o -= (self.lr * w2_update)        \n",
    "        self.weight_h -= (self.lr * w0_update)\n",
    "        self.bias_o -= (self.lr * b2_update)        \n",
    "        self.bias_h -= (self.lr * b0_update)\n",
    "\n",
    "        \n",
    "    def plot_MSE(self):\n",
    "        y = [self.loss_list[i] for i in range(len(self.loss_list))]\n",
    "        x = [x for x in range(len(y))]\n",
    "        plt.plot(x, y)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.title('MSE of the NN')\n",
    "        plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "90e71c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13122523 0.90812181 0.88557497 0.09602993]]\n"
     ]
    }
   ],
   "source": [
    "nnl = MyNeuralNet(2, 25, 1)\n",
    "nnl.init_weights()\n",
    "\n",
    "\n",
    "input_signal = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "target_signal = np.array([[0], [1], [1], [0]]).T\n",
    "\n",
    "# print(np.shape(target_signal))\n",
    "for epochs in range(1000):\n",
    "    ff = nnl.feedforward(input_signal)\n",
    "    gradients = nnl.back_propagation(*ff,target_signal, 4)\n",
    "    nnl.update_weights(*gradients)\n",
    "print(nnl.predict(input_signal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
