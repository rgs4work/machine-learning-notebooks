{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc17ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0becf62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_l1\n",
      "[[ 0.69858327 -0.30344136]\n",
      " [-0.70299227 -0.42011878]\n",
      " [ 0.01695842 -0.43007155]]\n",
      "bias_l1\n",
      "[[-0.07624925]\n",
      " [ 0.55471414]\n",
      " [ 0.05761911]]\n",
      "weight_l2\n",
      "[[ 0.19279515  0.50844163 -0.0339121 ]\n",
      " [-0.37761449 -0.30753624  0.25153619]\n",
      " [-0.12678089 -0.45187239 -0.05443081]]\n",
      "bias_l2\n",
      "[[-0.44169107]\n",
      " [-0.29700711]\n",
      " [-0.44188109]]\n",
      "weight_l3\n",
      "[[-0.22563614  0.55779625  0.06987352]]\n",
      "bias_l3\n",
      "[[0.56521095]]\n",
      "output_l0\n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "output_l1\n",
      "[[-0.07624925 -0.37969061  0.62233402  0.31889266]\n",
      " [ 0.55471414  0.13459536 -0.14827812 -0.5683969 ]\n",
      " [ 0.05761911 -0.37245244  0.07457752 -0.35549403]]\n",
      "output_l2\n",
      "[[-0.17630577 -0.43382904 -0.39962794 -0.65715121]\n",
      " [-0.4243157  -0.28870866 -0.46764961 -0.33204256]\n",
      " [-0.6860104  -0.43429062 -0.45783768 -0.20611789]]\n",
      "output_l3\n",
      "[[0.32037624 0.47171244 0.36253753 0.51387374]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, layers, activations):\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "\n",
    "        self.params = self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        '''Creates weights for the Neural Net'''\n",
    "        params = {}\n",
    "\n",
    "        for n in range(1, len(self.layers)):\n",
    "            edge = np.sqrt(1/self.layers[n-1])\n",
    "\n",
    "            # shape (layer, Previous Layer)\n",
    "            params[\"weight_l\" + str(n)] = np.random.uniform(-edge,\n",
    "                                                            edge, (self.layers[n], self.layers[n-1]))\n",
    "\n",
    "            # Bias shape(h,1) - only 1 Bias neuron per layer\n",
    "            params[\"bias_l\" + str(n)] = np.random.uniform(-edge,\n",
    "                                                          edge, (self.layers[n], 1))\n",
    "\n",
    "        return params\n",
    "\n",
    "    def feedforward(self, input_signal):\n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "\n",
    "        self.params[\"output_l0\"] = input_signal\n",
    "\n",
    "        for n in range(1, len(self.layers)):\n",
    "            self.params[\"output_l\" + str(n)] = (np.dot(self.params[\"weight_l\" + str(\n",
    "                n)], self.params[\"output_l\" + str(n-1)]) + self.params[\"bias_l\" + str(n)])\n",
    "\n",
    "\n",
    "nn = NeuralNet(layers=[2, 3, 3, 1], activations=[\"Relu\", \"Relu\", \"Linear\"])\n",
    "\n",
    "input_signal = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "nn.feedforward(input_signal)\n",
    " \n",
    "print(\"\\n\".join(\"{}\\n{}\".format(k, v) for k, v in nn.params.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(f):\n",
    "    return np.multiply(f, (1 - f))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(f):\n",
    "    return 1 - np.power(np.tanh(f), 2)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(int)\n",
    "\n",
    "\n",
    "def noneActivation(x):\n",
    "    return x    \n",
    "\n",
    "\n",
    "def noneActivation_derivative(z):\n",
    "    return 1    \n",
    "\n",
    "\n",
    "def mean_squared_error(target, output):\n",
    "    # print('\\n Prediction', output)\n",
    "    # print(target)\n",
    "    return np.square(target - output).mean()\n",
    "\n",
    "\n",
    "class MyNeuralNet:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons):\n",
    "        \n",
    "        # Neural Net Layers and Nodes\n",
    "        self.i = input_neurons  #\n",
    "        self.h = hidden_neurons  #\n",
    "        self.o = output_neurons  #\n",
    "        \n",
    "        # Activation functions and derivatives\n",
    "        self.activation_h =  relu\n",
    "        self.activation_o =  noneActivation\n",
    "\n",
    "        self.activation_derivative_h =  relu_derivative\n",
    "        self.activation_derivative_o =  noneActivation_derivative\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_function = mean_squared_error\n",
    "        self.loss = 0\n",
    "        self.loss_list = []\n",
    "        \n",
    "        # Optimizing\n",
    "        self.lr = 0.001\n",
    "        self.optimizer = 'ADAM'\n",
    "        \n",
    "        # ADAM Optimizer variables\n",
    "        self.l2_m = 0\n",
    "        self.l1_m = 0\n",
    "        self.l3_m = 0\n",
    "        \n",
    "        self.l2_v = 0\n",
    "        self.l1_v = 0\n",
    "        self.l3_v = 0\n",
    "\n",
    "        self.lb2_m = 0\n",
    "        self.lb1_m = 0\n",
    "        self.lb3_m = 0\n",
    "\n",
    "        self.lb2_v = 0\n",
    "        self.lb1_v = 0\n",
    "        self.lb3_v = 0\n",
    "\n",
    "        self.t = 0\n",
    "        \n",
    "    \n",
    "    def init_weights(self):     \n",
    "        '''Creates weights for the Neural Net'''\n",
    "        # shape (layer, Previous Layer)\n",
    "        self.weight_h =  np.random.uniform(-np.sqrt(1/self.i) , np.sqrt(1/self.i) ,(self.h, self.i))\n",
    "        self.weight_o = np.random.uniform(-np.sqrt(1/self.h) , np.sqrt(1/self.h) ,(self.o, self.h))\n",
    "        # Bias shape(h,1) - only 1 Bias neuron per layer\n",
    "        self.bias_h =  np.random.uniform(-np.sqrt(1/self.i) , np.sqrt(1/self.i) ,(self.h,1))\n",
    "        self.bias_o =  np.random.uniform(-np.sqrt(1/self.h) , np.sqrt(1/self.h) ,(self.o, 1))\n",
    "\n",
    "\n",
    "    def feedforward(self, input_signal):\n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "        # dot((R1,C1),(R2,C2)) output shape: (R1,C2), C1 == R2\n",
    "        output_i = input_signal\n",
    "#         output_i = np.multiply(input_signal,np.random.binomial(1, 1, input_signal.shape)) / np.full(input_signal.shape,0.9)\n",
    "                               \n",
    "        output_h = self.activation_h(np.dot(self.weight_h, output_i) + self.bias_h)\n",
    "#         output_h = np.multiply(output_h, np.random.binomial(1, 1, output_h.shape)) / np.full(output_h.shape,1) # DROPOUT\n",
    "       \n",
    "        output_o = self.activation_o(np.dot(self.weight_o, output_h) + self.bias_o)\n",
    "\n",
    "        return output_i, output_h, output_o #returns tuple\n",
    "    \n",
    "    \n",
    "\n",
    "    def back_propagation(self, output_i, output_h, output_o, target, batch_size):   \n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "        # Target shape : ( output layer size , batch_size)\n",
    "        # for graph\n",
    "        self.loss = self.loss_function(target, output_o)\n",
    "        if random.uniform(0, 1) > 0.05:\n",
    "            self.loss_list.append(self.loss)  \n",
    "        \n",
    "\n",
    "        error_o = -(2/len(target))*(target - output_o)\n",
    "       \n",
    "        delta_o =  error_o * self.activation_derivative_o(output_o) \n",
    "        \n",
    "        grad_o = np.dot(delta_o,  output_h.T) / batch_size     # Correct\n",
    "        grad_bias_o = np.mean(delta_o, axis=1, keepdims=True)   # Correct \n",
    "        \n",
    "  \n",
    "        delta_h = np.dot(self.weight_o.T, delta_o) * self.activation_derivative_h(output_h)        \n",
    "        \n",
    "        grad_h = np.dot(delta_h, output_i.T) / batch_size    # Correct\n",
    "        grad_bias_h = np.mean(delta_h, axis=1, keepdims=True)  # Correct  \n",
    "     \n",
    "        return grad_h ,grad_o, grad_bias_h, grad_bias_o\n",
    "\n",
    "    \n",
    "    \n",
    "    def update_weights(self, grad_h ,grad_o, grad_bias_h, grad_bias_o):\n",
    "        '''Returns (weight_h, weight_o, weight_bias_h,  weight_bias_o)'''             \n",
    "                \n",
    "        if self.optimizer == 'SGD':\n",
    "            self.sgd_optimizer( grad_h, grad_o, grad_bias_h, grad_bias_o)\n",
    "\n",
    "        elif self.optimizer == 'ADAM':\n",
    "            self.adam_optimizer(grad_h, grad_o, grad_bias_h, grad_bias_o)\n",
    "\n",
    "\n",
    "    def predict(self, input_signal):\n",
    "        \"\"\" Input signal should be 2D array shape : (Rows = input layer size (self.i) , Columns = batch_size)\"\"\"\n",
    "        # dot((R1,C1),(R2,C2)) output shape: (R1,C2), C1 == R2\n",
    "        \n",
    "        output_i = input_signal\n",
    "        output_h = self.activation_h(np.dot(self.weight_h, output_i) + self.bias_h)\n",
    "        output_o = self.activation_o(np.dot(self.weight_o, output_h) + self.bias_o)\n",
    "\n",
    "        return output_o #returns tuple\n",
    "    \n",
    "#         '''Returns only Output Layer array'''\n",
    "#         _, _, output_o = self.feedforward(input_signal)\n",
    "#         return output_o\n",
    "    \n",
    "\n",
    "        \n",
    "    def sgd_optimizer(self, grad_h, grad_o, grad_bias_h, grad_bias_o):\n",
    "        # Update Weights\n",
    "        self.weight_o = self.weight_o - self.lr * grad_o\n",
    "        self.weight_h = self.weight_h - self.lr * grad_h\n",
    "\n",
    "        self.bias_h = self.bias_h - self.lr * grad_bias_h\n",
    "        self.bias_o = self.bias_o - self.lr * grad_bias_o\n",
    "\n",
    "\n",
    "    def adam_optimizer(self, grad_h, grad_o, grad_bias_h, grad_bias_o):\n",
    "        decay_rate_1 = 0.9\n",
    "        decay_rate_2 = 0.999\n",
    "        epsilon = 10**(-8)\n",
    "\n",
    "        g2 = grad_o        \n",
    "        g0 = grad_h\n",
    "\n",
    "        gb2 = grad_bias_o\n",
    "        gb0 = grad_bias_h\n",
    "\n",
    "        self.t += 1  # Increment Time Step\n",
    "\n",
    "        # Computing 1st and 2nd moment for each layer\n",
    "        self.l3_m = self.l3_m * decay_rate_1 + (1 - decay_rate_1) * g2\n",
    "        self.l1_m = self.l1_m * decay_rate_1 + (1 - decay_rate_1) * g0\n",
    "        \n",
    "        self.l3_v = self.l3_v * decay_rate_2 + (1 - decay_rate_2) * np.square(g2)        \n",
    "        self.l1_v = self.l1_v * decay_rate_2 + (1 - decay_rate_2) * np.square(g0)\n",
    "        \n",
    "        self.lb3_m = self.lb3_m * decay_rate_1 + (1 - decay_rate_1) * gb2        \n",
    "        self.lb1_m = self.lb1_m * decay_rate_1 + (1 - decay_rate_1) * gb0\n",
    "        \n",
    "        self.lb3_v = self.lb3_v * decay_rate_2 + (1 - decay_rate_2) * np.square(gb2)        \n",
    "        self.lb1_v = self.lb1_v * decay_rate_2 + (1 - decay_rate_2) * np.square(gb0)\n",
    "\n",
    "        # Computing bias-corrected moment\n",
    "        l3_m_corrected = self.l3_m / (1 - (decay_rate_1 ** self.t))\n",
    "        l3_v_corrected = self.l3_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        l1_m_corrected = self.l1_m / (1 - (decay_rate_1 ** self.t))\n",
    "        l1_v_corrected = self.l1_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        lb3_m_corrected = self.lb3_m / (1 - (decay_rate_1 ** self.t))\n",
    "        lb3_v_corrected = self.lb3_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        lb1_m_corrected = self.lb1_m / (1 - (decay_rate_1 ** self.t))\n",
    "        lb1_v_corrected = self.lb1_v / (1 - (decay_rate_2 ** self.t))\n",
    "\n",
    "        # Update Weights\n",
    "        w2_update = l3_m_corrected / (np.sqrt(l3_v_corrected) + epsilon)        \n",
    "        w0_update = l1_m_corrected / (np.sqrt(l1_v_corrected) + epsilon)\n",
    "        b2_update = lb3_m_corrected / (np.sqrt(lb3_v_corrected) + epsilon)        \n",
    "        b0_update = lb1_m_corrected / (np.sqrt(lb1_v_corrected) + epsilon)\n",
    "\n",
    "        self.weight_o -= (self.lr * w2_update)        \n",
    "        self.weight_h -= (self.lr * w0_update)\n",
    "        self.bias_o -= (self.lr * b2_update)        \n",
    "        self.bias_h -= (self.lr * b0_update)\n",
    "\n",
    "        \n",
    "    def plot_MSE(self):\n",
    "        y = [self.loss_list[i] for i in range(len(self.loss_list))]\n",
    "        x = [x for x in range(len(y))]\n",
    "        plt.plot(x, y)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.title('MSE of the NN')\n",
    "        plt.show()    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
